{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2dba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/14 12:44:34 INFO mlflow.tracking.fluent: Experiment with name 'fraud_detection_v1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MLflow Tracking URI: file:../mlruns\n",
      " Artifacts Temp Folder: ./temp_artifacts/\n",
      " Environment Ready.\n"
     ]
    }
   ],
   "source": [
    "# Imports & Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn & XGBoost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, \n",
    "                             average_precision_score, roc_auc_score, \n",
    "                             f1_score, precision_score, recall_score)\n",
    "import xgboost as xgb\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# --- 1. Suppress Warnings (Clean Output) ---\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- 2. Configure Paths (Industry Standard) ---\n",
    "# Set tracking URI to the PARENT directory (outside notebooks folder)\n",
    "# \"file:../mlruns\" tells MLflow to create the folder one level up\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "\n",
    "# Create a temporary folder for artifacts (images) if it doesn't exist\n",
    "ARTIFACT_PATH = \"temp_artifacts\"\n",
    "os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "# --- 3. Experiment Setup ---\n",
    "EXPERIMENT_NAME = \"fraud_detection_v1\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\" MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\" Artifacts Temp Folder: ./{ARTIFACT_PATH}/\")\n",
    "print(\" Environment Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0cba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Set          Shape                Fraud Rate\n",
      "------------------------------------------------------------\n",
      "Train        (170884, 54)         0.211%\n",
      "Validation   (56961, 54)          0.100%\n",
      "Test         (56962, 54)          0.132%\n",
      "============================================================\n",
      "✓ Data loaded successfully.\n",
      "\n",
      "  Note: Test set will ONLY be used for final champion model evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-Processed Data\n",
    "DATA_PATH = \"../data/processed/\"\n",
    "\n",
    "# Load train, validation, and test (already split in Notebook 02)\n",
    "train_df = pd.read_parquet(DATA_PATH + \"train_processed.parquet\")\n",
    "val_df = pd.read_parquet(DATA_PATH + \"val_processed.parquet\")\n",
    "test_df = pd.read_parquet(DATA_PATH + \"test_processed.parquet\")\n",
    "\n",
    "# Quick validation\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Set':<12} {'Shape':<20} {'Fraud Rate'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Train':<12} {str(train_df.shape):<20} {train_df['is_fraud'].mean()*100:.3f}%\")\n",
    "print(f\"{'Validation':<12} {str(val_df.shape):<20} {val_df['is_fraud'].mean()*100:.3f}%\")\n",
    "print(f\"{'Test':<12} {str(test_df.shape):<20} {test_df['is_fraud'].mean()*100:.3f}%\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Data loaded successfully.\")\n",
    "print(\"\\n  Note: Test set will ONLY be used for final champion model evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5806aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE/TARGET SPLIT SUMMARY\n",
      "============================================================\n",
      "Features: 49 columns\n",
      "------------------------------------------------------------\n",
      "Set          X Shape              y Shape\n",
      "------------------------------------------------------------\n",
      "Train        (170884, 49)         (170884,)\n",
      "Validation   (56961, 49)          (56961,)\n",
      "Test         (56962, 49)          (56962,)\n",
      "============================================================\n",
      " Features and Target separated.\n"
     ]
    }
   ],
   "source": [
    "# Separate Features and Target\n",
    "\n",
    "# Metadata columns - keep for analysis but exclude from training\n",
    "METADATA_COLS = ['transaction_id', 'user_id', 'merchant_id', 'timestamp']\n",
    "\n",
    "# Target column\n",
    "TARGET = 'is_fraud'\n",
    "\n",
    "# Feature columns = Everything except metadata and target\n",
    "feature_cols = [col for col in train_df.columns if col not in METADATA_COLS + [TARGET]]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Separate X and y for ALL datasets\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# Training set (for model fitting)\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Validation set (for model comparison & hyperparameter tuning)\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df[TARGET]\n",
    "\n",
    "# Test set (ONLY for final champion model evaluation)\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "# Validation\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE/TARGET SPLIT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Features: {len(feature_cols)} columns\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Set':<12} {'X Shape':<20} {'y Shape'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Train':<12} {str(X_train.shape):<20} {str(y_train.shape)}\")\n",
    "print(f\"{'Validation':<12} {str(X_val.shape):<20} {str(y_val.shape)}\")\n",
    "print(f\"{'Test':<12} {str(X_test.shape):<20} {str(y_test.shape)}\")\n",
    "print(\"=\"*60)\n",
    "print(\" Features and Target separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e15f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation functions ready.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Helper Functions (Clean Version)\n",
    "# No changes needed - functions are generic\n",
    "\n",
    "def evaluate_model(model, X, y, model_name=\"Model\"):\n",
    "    \"\"\"Calculate and return all evaluation metrics.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1_score': f1_score(y, y_pred),\n",
    "        'roc_auc': roc_auc_score(y, y_prob),\n",
    "        'pr_auc': average_precision_score(y, y_prob)\n",
    "    }\n",
    "    return metrics, y_pred, y_prob\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix and return the Figure object.\n",
    "    Does NOT save to disk automatically.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix: {model_name}')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\" Evaluation functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a34f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run: Baseline_LogReg_Balanced\n",
      "  Training on: train_df\n",
      "  Evaluating on: val_df (NOT test_df)\n",
      "\n",
      "Validation Metrics:\n",
      "  precision: 0.0370\n",
      "  recall: 0.9123\n",
      "  f1_score: 0.0711\n",
      "  roc_auc: 0.9899\n",
      "  pr_auc: 0.7830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/14 12:45:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/14 12:45:26 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc259f1723224bc486f490e9e913dcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete. Evaluated on VALIDATION set.\n",
      "Cleaned up: temp_artifacts/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Baseline Experiment - Logistic Regression (Evaluate on VALIDATION set)\n",
    "\n",
    "# Define Preprocessor\n",
    "categorical_cols = ['card_tier', 'transaction_channel']\n",
    "numerical_cols = [col for col in X_train.columns if col not in categorical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', RobustScaler(), numerical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Setup Temp Directory\n",
    "ARTIFACT_PATH = \"temp_artifacts\"\n",
    "os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "run_name = \"Baseline_LogReg_Balanced\"\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        print(f\"Starting Run: {run_name}\")\n",
    "        print(\"  Training on: train_df\")\n",
    "        print(\"  Evaluating on: val_df (NOT test_df)\")\n",
    "        \n",
    "        # --- Define & Train Pipeline ---\n",
    "        pipeline = ImbPipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000))\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # --- Evaluate on VALIDATION set ---\n",
    "        metrics, y_pred, y_prob = evaluate_model(pipeline, X_val, y_val)\n",
    "        \n",
    "        # --- Log Metrics ---\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "            mlflow.log_metric(k, v)\n",
    "            \n",
    "        # --- Log Params ---\n",
    "        mlflow.log_param(\"model_type\", \"Logistic Regression\")\n",
    "        mlflow.log_param(\"eval_set\", \"validation\")  # Track which set was used\n",
    "        \n",
    "        # --- Handle Artifacts ---\n",
    "        fig = plot_confusion_matrix(y_val, y_pred, model_name=run_name)\n",
    "        temp_file = f\"{ARTIFACT_PATH}/confusion_matrix_{run_name}.png\"\n",
    "        fig.savefig(temp_file)\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(temp_file)\n",
    "        \n",
    "        # --- Log Model ---\n",
    "        input_example = X_train.iloc[:5]\n",
    "        signature = infer_signature(input_example, pipeline.predict(input_example))\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline, \n",
    "            artifact_path=\"model\",\n",
    "            signature=signature, \n",
    "            input_example=input_example\n",
    "        )\n",
    "        print(f\"Run Complete. Evaluated on VALIDATION set.\")\n",
    "\n",
    "finally:\n",
    "    if os.path.exists(ARTIFACT_PATH):\n",
    "        shutil.rmtree(ARTIFACT_PATH)\n",
    "        print(f\"Cleaned up: {ARTIFACT_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a797b691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run: Challenger_XGB_SMOTE\n",
      "  Training on: train_df\n",
      "  Evaluating on: val_df (NOT test_df)\n",
      "\n",
      "Validation Metrics:\n",
      "  precision: 0.8723\n",
      "  recall: 0.7193\n",
      "  f1_score: 0.7885\n",
      "  roc_auc: 0.9786\n",
      "  pr_auc: 0.7598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/14 12:45:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/14 12:46:15 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6f4fd6394245d78d8e23b8e76b7599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete. Evaluated on VALIDATION set.\n",
      "Cleaned up: temp_artifacts/\n"
     ]
    }
   ],
   "source": [
    "# Challenger Experiment - XGBoost + SMOTE (Evaluate on VALIDATION set)\n",
    "\n",
    "ARTIFACT_PATH = \"temp_artifacts\"\n",
    "os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "run_name = \"Challenger_XGB_SMOTE\"\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        print(f\"Starting Run: {run_name}\")\n",
    "        print(\"  Training on: train_df\")\n",
    "        print(\"  Evaluating on: val_df (NOT test_df)\")\n",
    "        \n",
    "        # --- Define Pipeline ---\n",
    "        pipeline_xgb = ImbPipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('smote', SMOTE(random_state=42, sampling_strategy=0.1)),\n",
    "            ('classifier', xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='aucpr',\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # --- Log Parameters ---\n",
    "        mlflow.log_param(\"model_type\", \"XGBoost\")\n",
    "        mlflow.log_param(\"smote_strategy\", \"0.1\")\n",
    "        mlflow.log_param(\"n_estimators\", 100)\n",
    "        mlflow.log_param(\"eval_set\", \"validation\")\n",
    "        \n",
    "        # --- Train ---\n",
    "        pipeline_xgb.fit(X_train, y_train)\n",
    "        \n",
    "        # --- Evaluate on VALIDATION set ---\n",
    "        metrics, y_pred, y_prob = evaluate_model(pipeline_xgb, X_val, y_val)\n",
    "        \n",
    "        # --- Log Metrics ---\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "            mlflow.log_metric(k, v)\n",
    "            \n",
    "        # --- Artifacts ---\n",
    "        fig = plot_confusion_matrix(y_val, y_pred, model_name=run_name)\n",
    "        temp_file = f\"{ARTIFACT_PATH}/confusion_matrix_{run_name}.png\"\n",
    "        fig.savefig(temp_file)\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(temp_file)\n",
    "        \n",
    "        # --- Log Model ---\n",
    "        input_example = X_train.iloc[:5]\n",
    "        signature = infer_signature(input_example, pipeline_xgb.predict(input_example))\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline_xgb, \n",
    "            artifact_path=\"model\",\n",
    "            signature=signature, \n",
    "            input_example=input_example\n",
    "        )\n",
    "        print(f\"Run Complete. Evaluated on VALIDATION set.\")\n",
    "\n",
    "finally:\n",
    "    if os.path.exists(ARTIFACT_PATH):\n",
    "        shutil.rmtree(ARTIFACT_PATH)\n",
    "        print(f\"Cleaned up: {ARTIFACT_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc40a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated scale_pos_weight: 473.68\n",
      "Starting Run: XGB_ClassWeights_Optimized\n",
      "  Training on: train_df\n",
      "  Evaluating on: val_df (NOT test_df)\n",
      "\n",
      "Validation Metrics:\n",
      "  precision: 0.8269\n",
      "  recall: 0.7544\n",
      "  f1_score: 0.7890\n",
      "  roc_auc: 0.9771\n",
      "  pr_auc: 0.7492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/14 12:46:37 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/14 12:46:56 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad75c32819246278deea4dae2070323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Complete. Evaluated on VALIDATION set.\n",
      "Cleaned up: temp_artifacts/\n"
     ]
    }
   ],
   "source": [
    "# Optimization - XGBoost with Class Weights (Evaluate on VALIDATION set)\n",
    "\n",
    "ARTIFACT_PATH = \"temp_artifacts\"\n",
    "os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "run_name = \"XGB_ClassWeights_Optimized\"\n",
    "\n",
    "# Calculate scale_pos_weight from TRAINING data only\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"Calculated scale_pos_weight: {scale_weight:.2f}\")\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        print(f\"Starting Run: {run_name}\")\n",
    "        print(\"  Training on: train_df\")\n",
    "        print(\"  Evaluating on: val_df (NOT test_df)\")\n",
    "        \n",
    "        # --- Define Pipeline (NO SMOTE) ---\n",
    "        pipeline_weighted = ImbPipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='aucpr',\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                scale_pos_weight=scale_weight,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # --- Log Params ---\n",
    "        mlflow.log_param(\"model_type\", \"XGBoost\")\n",
    "        mlflow.log_param(\"technique\", \"Class Weights\")\n",
    "        mlflow.log_param(\"scale_pos_weight\", round(scale_weight, 2))\n",
    "        mlflow.log_param(\"eval_set\", \"validation\")\n",
    "        \n",
    "        # --- Train ---\n",
    "        pipeline_weighted.fit(X_train, y_train)\n",
    "        \n",
    "        # --- Evaluate on VALIDATION set ---\n",
    "        metrics, y_pred, y_prob = evaluate_model(pipeline_weighted, X_val, y_val)\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "            mlflow.log_metric(k, v)\n",
    "            \n",
    "        # --- Artifacts ---\n",
    "        fig = plot_confusion_matrix(y_val, y_pred, model_name=run_name)\n",
    "        temp_file = f\"{ARTIFACT_PATH}/confusion_matrix_{run_name}.png\"\n",
    "        fig.savefig(temp_file)\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(temp_file)\n",
    "        \n",
    "        # --- Log Model ---\n",
    "        input_example = X_train.iloc[:5]\n",
    "        signature = infer_signature(input_example, pipeline_weighted.predict(input_example))\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline_weighted, \n",
    "            artifact_path=\"model\",\n",
    "            signature=signature, \n",
    "            input_example=input_example\n",
    "        )\n",
    "        print(f\"Run Complete. Evaluated on VALIDATION set.\")\n",
    "\n",
    "finally:\n",
    "    if os.path.exists(ARTIFACT_PATH):\n",
    "        shutil.rmtree(ARTIFACT_PATH)\n",
    "        print(f\"Cleaned up: {ARTIFACT_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ec053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHAMPION MODEL SELECTION\n",
      "============================================================\n",
      "\n",
      "CHAMPION MODEL: Baseline_LogReg_Balanced\n",
      "   Run ID: 9c4f75408db147cfa96be3072dc0ba57\n",
      "   Validation PR-AUC: 0.7830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efa6826b3e243b88fab8d6438750f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bed86a22194d5a99adf1a5ad48bdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL TEST SET EVALUATION (First Time Touching Test Data!)\n",
      "============================================================\n",
      "\n",
      "TEST SET METRICS (Unbiased Performance Estimate):\n",
      "----------------------------------------\n",
      "  precision: 0.0439\n",
      "  recall: 0.9333\n",
      "  f1_score: 0.0839\n",
      "  roc_auc: 0.9928\n",
      "  pr_auc: 0.7895\n",
      "\n",
      "Test metrics logged to MLflow with 'test_' prefix.\n",
      "\n",
      "============================================================\n",
      "VALIDATION vs TEST COMPARISON\n",
      "============================================================\n",
      "Metric            Validation         Test   Difference\n",
      "------------------------------------------------------------\n",
      "precision             0.0370       0.0439      +0.0070 OK\n",
      "recall                0.9123       0.9333      +0.0211 OK\n",
      "f1_score              0.0711       0.0839      +0.0128 OK\n",
      "roc_auc               0.9899       0.9928      +0.0029 OK\n",
      "pr_auc                0.7830       0.7895      +0.0066 OK\n",
      "============================================================\n",
      "\n",
      "Insight: Small differences indicate good generalization.\n",
      "   Large drops may indicate overfitting to validation set.\n",
      "\n",
      "Model ready for deployment!\n",
      "   Deploy using Run ID: 9c4f75408db147cfa96be3072dc0ba57\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CHAMPION MODEL SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Compare all runs based on VALIDATION metrics\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id]\n",
    ")\n",
    "\n",
    "# Sort by PR-AUC (Best metric for imbalance)\n",
    "best_run = runs.sort_values(\"metrics.pr_auc\", ascending=False).iloc[0]\n",
    "\n",
    "best_run_id = best_run[\"run_id\"]\n",
    "best_model_name = best_run[\"tags.mlflow.runName\"]\n",
    "best_val_score = best_run[\"metrics.pr_auc\"]\n",
    "\n",
    "print(f\"\\nCHAMPION MODEL: {best_model_name}\")\n",
    "print(f\"   Run ID: {best_run_id}\")\n",
    "print(f\"   Validation PR-AUC: {best_val_score:.4f}\")\n",
    "\n",
    "# 2. Load the Champion Model\n",
    "champion_model = mlflow.sklearn.load_model(f\"runs:/{best_run_id}/model\")\n",
    "\n",
    "# 3. FINAL EVALUATION ON TEST SET (First and Only Time!)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SET EVALUATION (First Time Touching Test Data!)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_metrics, y_test_pred, y_test_prob = evaluate_model(\n",
    "    champion_model, X_test, y_test, model_name=best_model_name\n",
    ")\n",
    "\n",
    "print(\"\\nTEST SET METRICS (Unbiased Performance Estimate):\")\n",
    "print(\"-\"*40)\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# 4. Log Test Metrics to the Champion Run\n",
    "with mlflow.start_run(run_id=best_run_id):\n",
    "    for k, v in test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{k}\", v)  # Prefix with 'test_' to distinguish\n",
    "    mlflow.log_param(\"final_eval_set\", \"test\")\n",
    "    \n",
    "print(\"\\nTest metrics logged to MLflow with 'test_' prefix.\")\n",
    "\n",
    "# 5. Compare Validation vs Test Performance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION vs TEST COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<15} {'Validation':>12} {'Test':>12} {'Difference':>12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "val_metrics_keys = ['precision', 'recall', 'f1_score', 'roc_auc', 'pr_auc']\n",
    "for k in val_metrics_keys:\n",
    "    val_score = best_run[f\"metrics.{k}\"]\n",
    "    test_score = test_metrics[k]\n",
    "    diff = test_score - val_score\n",
    "    status = \"OK\" if abs(diff) < 0.05 else \"Warning\"\n",
    "    print(f\"{k:<15} {val_score:>12.4f} {test_score:>12.4f} {diff:>+12.4f} {status}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInsight: Small differences indicate good generalization.\")\n",
    "print(\"   Large drops may indicate overfitting to validation set.\")\n",
    "\n",
    "# 6. Feature Importance (Same as before)\n",
    "if \"XGB\" in best_model_name:\n",
    "    xgb_model = champion_model.named_steps['classifier']\n",
    "    preprocessor = champion_model.named_steps['preprocessor']\n",
    "    \n",
    "    cat_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "    all_feature_names = list(cat_names) + list(numerical_cols)\n",
    "    \n",
    "    importances = xgb_model.feature_importances_\n",
    "    \n",
    "    feat_df = pd.DataFrame({\n",
    "        'Feature': all_feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feat_df, x='Importance', y='Feature', palette='viridis')\n",
    "    plt.title(f\"Top 10 Fraud Drivers ({best_model_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nModel ready for deployment!\")\n",
    "print(f\"   Deploy using Run ID: {best_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b102b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.3.5\n",
      "SHAP version: 0.50.0\n",
      "SHAP imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "print(\"SHAP imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30b165ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining Champion Model: Baseline_LogReg_Balanced\n",
      "  Using: Test set sample (100 transactions)\n",
      "  Using LinearExplainer for non-tree models\n",
      "Cleaned up: temp_artifacts/\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "ARTIFACT_PATH = \"temp_artifacts\"\n",
    "os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "preprocessor = champion_model.named_steps['preprocessor']\n",
    "classifier = champion_model.named_steps['classifier']\n",
    "\n",
    "# Use TEST set sample for SHAP (Champion model is now final)\n",
    "X_sample = X_test.sample(100, random_state=42)\n",
    "X_sample_transformed = preprocessor.transform(X_sample)\n",
    "\n",
    "print(f\"Explaining Champion Model: {best_model_name}\")\n",
    "print(f\"  Using: Test set sample (100 transactions)\")\n",
    "\n",
    "# Get feature names\n",
    "cat_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "all_names = list(cat_names) + list(numerical_cols)\n",
    "\n",
    "if any(model in best_model_name.upper() for model in ['RANDOMFOREST', 'XGB']):\n",
    "    print(\"  Using TreeExplainer (Fast)\")\n",
    "    explainer = shap.TreeExplainer(classifier)\n",
    "    shap_values = explainer.shap_values(X_sample_transformed)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # Summary Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_sample_transformed, \n",
    "                      feature_names=all_names, \n",
    "                      show=False)\n",
    "    \n",
    "    temp_file = f\"{ARTIFACT_PATH}/shap_summary_{best_run_id}.png\"\n",
    "    plt.savefig(temp_file, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"SHAP plot saved: {temp_file}\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_id=best_run_id):\n",
    "        mlflow.log_artifact(temp_file)\n",
    "    \n",
    "    print(\"SHAP Summary logged to MLflow.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_sample_transformed, \n",
    "                      feature_names=all_names)\n",
    "\n",
    "else:\n",
    "    print(\"  Using LinearExplainer for non-tree models\")\n",
    "\n",
    "# Cleanup\n",
    "if os.path.exists(ARTIFACT_PATH):\n",
    "    shutil.rmtree(ARTIFACT_PATH)\n",
    "    print(f\"Cleaned up: {ARTIFACT_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34098246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realtime-fraud-detection-mlops (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
